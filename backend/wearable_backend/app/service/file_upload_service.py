import os, shutil, tempfile, uuid
from pathlib import Path
from datetime import datetime
from fastapi import UploadFile, HTTPException
import asyncio
from concurrent.futures import ThreadPoolExecutor

from app.core.unzipper import extract_zip_to_temp
from app.core.db_to_json import db_to_json
from app.core.db_parser import parse_db_json_to_raw_data_by_day

from app.utils.preprocess import preprocess_health_json
from app.core.vector_store import save_daily_summaries_batch
from app.core.llm_analysis import run_llm_analysis

# ë¹„ë™ê¸° ì²˜ë¦¬ìš© Executor
executor = ThreadPoolExecutor(max_workers=4)

# ============================================================
# ZIP ì €ì¥ ê²½ë¡œ ì„¤ì •
# ============================================================
BASE_DIR = Path(__file__).resolve().parent.parent.parent  # backend/
ZIP_DATA_DIR = BASE_DIR / "zip_data"
UPLOADS_DIR = ZIP_DATA_DIR / "uploads"
EXTRACTED_DIR = ZIP_DATA_DIR / "extracted"

# ë””ë ‰í† ë¦¬ ìƒì„±
UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
EXTRACTED_DIR.mkdir(parents=True, exist_ok=True)


class FileUploadService:
    """
    ZIP/DB íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ì„œë¹„ìŠ¤ (ë‚ ì§œ ë° í”Œë«í¼ ì •ë³´ ê°œì„  ë²„ì „)

    ê°œì„  ì‚¬í•­:
    1. ë‚ ì§œ ì •ë³´ ì œëŒ€ë¡œ ì „ë‹¬ (date_int í™œìš©)
    2. í”Œë«í¼ ì •ë³´ ìë™ ê°ì§€ (Apple/Samsung)
    3. VectorDBì— ì •í™•í•œ ë‚ ì§œ ì €ì¥
    4. ZIP íŒŒì¼ í”„ë¡œì íŠ¸ í´ë”ì— ì˜êµ¬ ì €ì¥
    """

    @staticmethod
    def get_or_create_user_id(user_id: str | None):
        if not user_id or not user_id.strip():
            return str(uuid.uuid4())
        return user_id

    @staticmethod
    async def run_blocking(func, *args):
        """ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(executor, lambda: func(*args))

    @staticmethod
    def detect_platform(filename: str, db_json: dict) -> str:
        """
        í”Œë«í¼ ìë™ ê°ì§€

        Returns:
            "apple" or "samsung" or "unknown"
        """
        filename_lower = filename.lower()

        # âœ… íŒŒì¼ëª…ìœ¼ë¡œ ê°ì§€
        if "healthconnect" in filename_lower or "samsung" in filename_lower:
            return "samsung"
        elif (
            "export" in filename_lower
            or "apple" in filename_lower
            or "health" in filename_lower
        ):
            return "apple"

        # âœ… DB êµ¬ì¡°ë¡œ ê°ì§€ (Samsung Health Connect íŠ¹ì§•)
        if db_json:
            # Samsung Health ConnectëŠ” íŠ¹ì • í…Œì´ë¸” ì¡´ì¬
            samsung_tables = [
                "steps_record_table",
                "distance_record_table",
                "heart_rate_record_table",
            ]
            if all(table in db_json for table in samsung_tables):
                return "samsung"

            # Apple Health ExportëŠ” ë‹¤ë¥¸ êµ¬ì¡°
            apple_indicators = ["HealthKit", "HKQuantityTypeIdentifier"]
            # (ì¶”ê°€ ë¡œì§ í•„ìš” ì‹œ)

        return "unknown"

    async def process_file(
        self,
        file: UploadFile,
        user_id: str | None,
        difficulty: str,
        duration: int,
    ):
        user_id = self.get_or_create_user_id(user_id)

        # ì‚¬ìš©ìë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ë””ë ‰í† ë¦¬
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        user_short = user_id.replace("@", "_").replace(".", "_")

        temp_dir = str(EXTRACTED_DIR / f"{user_short}_{timestamp}")
        os.makedirs(temp_dir, exist_ok=True)

        temp_path = os.path.join(temp_dir, file.filename)

        try:
            print(f"[INFO] íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘: {file.filename}")

            # 1ï¸âƒ£ íŒŒì¼ ì €ì¥
            with open(temp_path, "wb") as buffer:
                buffer.write(await file.read())

            # ============================================================
            # ğŸ“Œ ìˆ˜ì •: ZIP/DB ëª¨ë‘ uploads/ í´ë”ì— ì›ë³¸ ì €ì¥
            # ============================================================
            original_save_name = f"{user_short}_{timestamp}_{file.filename}"
            original_save_path = UPLOADS_DIR / original_save_name
            shutil.copy2(temp_path, original_save_path)
            print(f"[INFO] ì›ë³¸ íŒŒì¼ ì €ì¥: {original_save_path}")

            # 2ï¸âƒ£ ZIP ë˜ëŠ” DB íŒë³„
            if file.filename.lower().endswith(".zip"):
                print("[INFO] ZIP íŒŒì¼ ì••ì¶• í•´ì œ ì¤‘...")
                db_path = await self.run_blocking(extract_zip_to_temp, temp_path)
            elif file.filename.lower().endswith(".db"):
                db_path = temp_path
            else:
                raise HTTPException(400, "ZIP ë˜ëŠ” DB íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

            if not db_path:
                raise HTTPException(500, "DB íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # 3ï¸âƒ£ DB â†’ JSON (ë¹„ë™ê¸° ì²˜ë¦¬)
            print("[INFO] DB íŒŒì‹± ì¤‘...")
            raw_db_json = await self.run_blocking(db_to_json, db_path)

            # âœ… ê°œì„ : í”Œë«í¼ ê°ì§€
            platform = self.detect_platform(file.filename, raw_db_json)
            print(f"[INFO] ê°ì§€ëœ í”Œë«í¼: {platform}")

            # 4ï¸âƒ£ ë‚ ì§œë³„ raw ì¶”ì¶œ
            print("[INFO] ë‚ ì§œë³„ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
            raw_by_day = await self.run_blocking(
                parse_db_json_to_raw_data_by_day, raw_db_json
            )

            if not raw_by_day:
                raise HTTPException(
                    500, "DB Parserê°€ ê±´ê°• ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                )

            total_days = len(raw_by_day)
            print(f"[INFO] ì´ {total_days}ì¼ì¹˜ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")

            # ë‚ ì§œ ë²”ìœ„ ì¶œë ¥
            dates = sorted(raw_by_day.keys())
            if dates:
                print(f"[INFO] ë‚ ì§œ ë²”ìœ„: {dates[0]} ~ {dates[-1]}")

            # 5ï¸âƒ£ ìµœì‹  ë‚ ì§œ ê²°ì •
            latest_date = max(raw_by_day.keys())
            latest_raw = raw_by_day[latest_date]

            # 6ï¸âƒ£ ìµœì‹  1ì¼ì¹˜ summary (ë¶„ì„ìš©)
            print("[INFO] ìµœì‹  ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
            latest_summary = await self.run_blocking(
                preprocess_health_json, latest_raw, latest_date, platform
            )

            # 7ï¸âƒ£ ì „ì²´ ë‚ ì§œ summary â†’ Vector DB ë°°ì¹˜ ì €ì¥
            print(f"[INFO] VectorDBì— {total_days}ì¼ì¹˜ ë°ì´í„° ë°°ì¹˜ ì €ì¥ ì¤‘...")

            all_summaries = []
            for date_int, raw in raw_by_day.items():
                daily_summary = await self.run_blocking(
                    preprocess_health_json,
                    raw,
                    date_int,
                    platform,
                )
                all_summaries.append(daily_summary)

            source = f"zip_{platform}"
            await self.run_blocking(
                save_daily_summaries_batch, all_summaries, user_id, source
            )

            print(
                f"[SUCCESS] {total_days}ì¼ì¹˜ ë°ì´í„° VectorDB ì €ì¥ ì™„ë£Œ (í”Œë«í¼: {platform})"
            )

            # 8ï¸âƒ£ LLM ë¶„ì„ (ìµœì‹  ë°ì´í„°ë§Œ)
            print("[INFO] LLM ë¶„ì„ ì‹¤í–‰ ì¤‘...")
            llm_result = await self.run_blocking(
                run_llm_analysis,
                latest_summary,
                user_id,
                difficulty,
                duration,
            )

            print("[SUCCESS] ë¶„ì„ ì™„ë£Œ")

            # ============================================================
            # ğŸ“Œ ìˆ˜ì •: ì €ì¥ ê²½ë¡œ ì •ë³´ ë¡œê·¸
            # ============================================================
            print(f"\n{'='*70}")
            print(f"ğŸ“¦ íŒŒì¼ ì €ì¥ ì •ë³´:")
            print(f"  â€¢ íŒŒì¼ íƒ€ì…: {file.filename.split('.')[-1].upper()}")
            print(f"  â€¢ ì›ë³¸ íŒŒì¼: {original_save_path}")
            print(f"  â€¢ ì••ì¶• í•´ì œ: {temp_dir}")
            print(f"  â€¢ í”Œë«í¼: {platform}")
            print(f"  â€¢ ë‚ ì§œ ë²”ìœ„: {dates[0]} ~ {dates[-1]}")
            print(f"{'='*70}\n")

            return {
                "message": "ZIP/DB ì—…ë¡œë“œ ë° ë¶„ì„ ì„±ê³µ",
                "user_id": user_id,
                "total_days_saved": total_days,
                "date_range": f"{dates[0]} ~ {dates[-1]}" if dates else "",
                "latest_date": latest_date,
                "platform": platform,
                "summary": latest_summary,
                "llm_result": llm_result,
                "file_info": {
                    "file_type": file.filename.split(".")[-1],
                    "original_path": str(original_save_path),
                    "extract_dir": temp_dir,
                },
            }

        except HTTPException:
            raise
        except Exception as e:
            print(f"[ERROR] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback

            traceback.print_exc()
            raise HTTPException(500, f"ZIP/DB ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        finally:
            # 9ï¸âƒ£ ì´ì „ ë°ì´í„° ì •ë¦¬ + í˜„ì¬ ë°ì´í„° ë³´ì¡´
            try:
                # 1. í˜„ì¬ ì‚¬ìš©ìì˜ ëª¨ë“  ì¶”ì¶œ ë””ë ‰í† ë¦¬ ì°¾ê¸°
                user_pattern = f"{user_short}_*"
                user_dirs = list(EXTRACTED_DIR.glob(user_pattern))

                # 2. í˜„ì¬ ë””ë ‰í† ë¦¬ ì œì™¸
                current_dir = Path(temp_dir)
                old_dirs = [d for d in user_dirs if d != current_dir]

                # 3. ì´ì „ ì¶”ì¶œ ë””ë ‰í† ë¦¬ ì‚­ì œ
                for old_dir in old_dirs:
                    print(f"[INFO] ì´ì „ ë°ì´í„° ì‚­ì œ: {old_dir.name}")
                    shutil.rmtree(old_dir)

                # ============================================================
                # ğŸ“Œ ìˆ˜ì •: ëª¨ë“  íŒŒì¼ íƒ€ì…ì— ëŒ€í•´ ì´ì „ ì›ë³¸ ì‚­ì œ
                # ============================================================
                # 4. ê°™ì€ ìœ ì €ì˜ ì´ì „ ì›ë³¸ íŒŒì¼ ì‚­ì œ (ZIP/DB ëª¨ë‘)
                file_pattern = f"{user_short}_*.*"  # ëª¨ë“  í™•ì¥ì
                old_files = list(UPLOADS_DIR.glob(file_pattern))

                # í˜„ì¬ íŒŒì¼ ì œì™¸
                current_file = UPLOADS_DIR / original_save_name
                old_files = [f for f in old_files if f != current_file]

                for old_file in old_files:
                    print(f"[INFO] ì´ì „ ì›ë³¸ íŒŒì¼ ì‚­ì œ: {old_file.name}")
                    old_file.unlink()

                print(f"[INFO] ìµœì‹  ë°ì´í„° ë³´ì¡´: {temp_dir}")
                print(f"[INFO] ìµœì‹  ì›ë³¸ ë³´ì¡´: {original_save_path}")

            except Exception as e:
                print(f"[WARN] ì´ì „ ë°ì´í„° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {str(e)}")
