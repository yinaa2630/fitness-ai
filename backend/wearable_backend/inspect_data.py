#!/usr/bin/env python3
"""
ë°ì´í„° ê²€ì‚¬ í†µí•© ìŠ¤í¬ë¦½íŠ¸ (VectorDB + ZIP ë¶„ì„)

ê¸°ëŠ¥:
1. VectorDB ë°ì´í„° ì¡°íšŒ (ì‚¬ìš©ìë³„, ë‚ ì§œë³„, ì¤‘ë³µ í™•ì¸ ë“±)
2. ZIP íŒŒì¼ ë‚´ë¶€ í…Œì´ë¸”/ì»¬ëŸ¼/ë°ì´í„° ë¶„ì„
3. ZIP â†’ ì •ì œ ë°ì´í„° ë³€í™˜ í™•ì¸

ì‚¬ìš©ë²•:
  python inspect_data.py --help
"""

import sys
import os
import json
from datetime import datetime

# ë°±ì—”ë“œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.abspath("."))

# .env íŒŒì¼ ë¡œë“œ (ì„ íƒì )
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass
except Exception:
    pass

from app.core.vector_store import collection


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================


def print_header(title):
    """í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * 100)
    print(f"  {title}")
    print("=" * 100)


def print_subheader(title):
    """ì„œë¸Œí—¤ë” ì¶œë ¥"""
    print(f"\n{'â”€'*60}")
    print(f"  {title}")
    print(f"{'â”€'*60}")


def format_data_value(key, value):
    """ë°ì´í„° ê°’ í¬ë§·íŒ…"""
    if isinstance(value, (int, float)):
        if key in ["weight", "bmi", "body_fat", "lean_body"]:
            return f"{value:.1f}"
        elif key in ["distance_km"]:
            return f"{value:.2f} km"
        elif key in ["steps", "flights"]:
            return f"{value:,}"
        elif key in ["active_calories", "total_calories", "calories_intake"]:
            return f"{value} kcal"
        elif key in ["sleep_hr"]:
            return f"{value} ì‹œê°„"
        elif key in ["sleep_min", "exercise_min"]:
            return f"{value} ë¶„"
        else:
            return str(value)
    return str(value)


# ============================================================
# 1. ZIP íŒŒì¼ ë¶„ì„ ê¸°ëŠ¥
# ============================================================


def inspect_zip(zip_path: str):
    """
    ZIP íŒŒì¼ ë‚´ ëª¨ë“  í…Œì´ë¸”ê³¼ ë°ì´í„° êµ¬ì¡° í™•ì¸

    Args:
        zip_path: ZIP íŒŒì¼ ê²½ë¡œ

    Returns:
        dict: DB JSON ë°ì´í„°
    """
    from app.core.unzipper import extract_zip_to_temp
    from app.core.db_to_json import db_to_json

    print_header(f"ğŸ“¦ ZIP íŒŒì¼ ë¶„ì„: {os.path.basename(zip_path)}")

    if not os.path.exists(zip_path):
        print(f"\nâŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {zip_path}")
        return None

    # 1) ZIP â†’ DB ê²½ë¡œ ì¶”ì¶œ
    db_path = extract_zip_to_temp(zip_path)
    print(f"\nâœ… DB ê²½ë¡œ: {db_path}")

    # 2) DB â†’ JSON ë³€í™˜
    db_json = db_to_json(db_path)

    print(f"ğŸ“‹ ì´ í…Œì´ë¸” ìˆ˜: {len(db_json)}ê°œ\n")

    # 3) í…Œì´ë¸”ë³„ ìš”ì•½
    table_summary = []
    for table_name, rows in db_json.items():
        row_count = len(rows)
        columns = list(rows[0].keys()) if rows else []
        table_summary.append(
            {"table": table_name, "rows": row_count, "columns": columns}
        )

    # ë ˆì½”ë“œ ìˆ˜ ê¸°ì¤€ ì •ë ¬
    table_summary.sort(key=lambda x: x["rows"], reverse=True)

    print(f"{'í…Œì´ë¸”ëª…':<45} {'ë ˆì½”ë“œ':<10} {'ì»¬ëŸ¼ ìˆ˜':<10}")
    print(f"{'-'*45} {'-'*10} {'-'*10}")

    for t in table_summary:
        status = "âœ…" if t["rows"] > 0 else "âš ï¸"
        print(f"{status} {t['table']:<43} {t['rows']:<10} {len(t['columns']):<10}")

    return db_json


def inspect_zip_table(
    zip_path: str, table_name: str = None, limit: int = 5, summary_only: bool = False
):
    """
    ZIP íŒŒì¼ ë‚´ íŠ¹ì • í…Œì´ë¸” ìƒì„¸ í™•ì¸

    Args:
        zip_path: ZIP íŒŒì¼ ê²½ë¡œ
        table_name: í…Œì´ë¸”ëª… (Noneì´ë©´ ë°ì´í„° ìˆëŠ” ëª¨ë“  í…Œì´ë¸”)
        limit: ìƒ˜í”Œ ë°ì´í„° ê°œìˆ˜
        summary_only: Trueë©´ í•µì‹¬ ë°ì´í„°ë§Œ í‘œì‹œ
    """
    from app.core.unzipper import extract_zip_to_temp
    from app.core.db_to_json import db_to_json
    from app.utils.preprocess import epoch_day_to_date_string

    print_header(f"ğŸ“¦ ZIP í…Œì´ë¸” ìƒì„¸: {os.path.basename(zip_path)}")

    db_path = extract_zip_to_temp(zip_path)
    db_json = db_to_json(db_path)

    if table_name:
        # íŠ¹ì • í…Œì´ë¸”ë§Œ
        tables_to_show = {table_name: db_json.get(table_name, [])}
        if not db_json.get(table_name):
            print(f"\nâŒ í…Œì´ë¸” '{table_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”: {list(db_json.keys())}")
            return
    else:
        # ë°ì´í„° ìˆëŠ” í…Œì´ë¸”ë§Œ
        tables_to_show = {k: v for k, v in db_json.items() if v}

    # í…Œì´ë¸”ë³„ í•µì‹¬ í•„ë“œ ë§¤í•‘
    table_key_fields = {
        "steps_record_table": ["local_date", "count"],
        "distance_record_table": ["local_date", "distance"],
        "heart_rate_record_table": ["local_date", "value"],
        "resting_heart_rate_record_table": ["local_date", "value"],
        "sleep_session_record_table": ["local_date", "start_time", "end_time"],
        "weight_record_table": ["local_date", "weight"],
        "height_record_table": ["local_date", "height"],
        "total_calories_burned_record_table": ["local_date", "energy"],
        "active_calories_burned_record_table": ["local_date", "energy"],
        "oxygen_saturation_record_table": ["local_date", "percentage"],
    }

    for tbl_name, rows in tables_to_show.items():
        print_subheader(f"ğŸ“ {tbl_name} ({len(rows)}ê°œ ë ˆì½”ë“œ)")

        if not rows:
            print("   (ë°ì´í„° ì—†ìŒ)")
            continue

        # ì»¬ëŸ¼ ì •ë³´
        columns = list(rows[0].keys())

        if not summary_only:
            print(f"\n   ğŸ“Œ ì»¬ëŸ¼ ({len(columns)}ê°œ):")
            print(f"      {columns}")

        # ë‚ ì§œ ê¸°ì¤€ ìµœì‹ ìˆœ ì •ë ¬ (local_date ë˜ëŠ” date ì»¬ëŸ¼)
        date_col = None
        if "local_date" in columns:
            date_col = "local_date"
        elif "date" in columns:
            date_col = "date"

        if date_col:
            sorted_rows = sorted(rows, key=lambda x: x.get(date_col, 0), reverse=True)
        else:
            sorted_rows = rows

        # ìš”ì•½ ëª¨ë“œ
        if summary_only:
            key_fields = table_key_fields.get(tbl_name, ["local_date"])
            print(f"\n   ğŸ“Š í•µì‹¬ ë°ì´í„° (ìµœì‹  {limit}ê°œ):\n")

            for i, row in enumerate(sorted_rows[:limit], 1):
                # ë‚ ì§œ ë³€í™˜
                local_date = row.get("local_date", 0)
                try:
                    date_str = epoch_day_to_date_string(local_date)
                except:
                    date_str = str(local_date)

                # í…Œì´ë¸”ë³„ í•µì‹¬ ê°’ ì¶”ì¶œ
                if tbl_name == "steps_record_table":
                    count = row.get("count", 0)
                    print(f"      [{i}] ğŸ“… {date_str} | ğŸ‘£ {count:,}ë³´")

                elif tbl_name == "distance_record_table":
                    distance = row.get("distance", 0)
                    print(f"      [{i}] ğŸ“… {date_str} | ğŸ“ {distance/1000:.2f}km")

                elif tbl_name in [
                    "heart_rate_record_table",
                    "resting_heart_rate_record_table",
                ]:
                    value = row.get("value", 0)
                    print(f"      [{i}] ğŸ“… {date_str} | â¤ï¸ {value}bpm")

                elif tbl_name == "sleep_session_record_table":
                    start = row.get("start_time", 0)
                    end = row.get("end_time", 0)
                    duration_min = (end - start) / 1000 / 60 if start and end else 0
                    duration_hr = duration_min / 60
                    print(
                        f"      [{i}] ğŸ“… {date_str} | ğŸ˜´ {duration_hr:.1f}ì‹œê°„ ({duration_min:.0f}ë¶„)"
                    )

                elif tbl_name == "weight_record_table":
                    weight = row.get("weight", 0) / 1000  # gram â†’ kg
                    print(f"      [{i}] ğŸ“… {date_str} | âš–ï¸ {weight:.1f}kg")

                elif tbl_name == "height_record_table":
                    height = row.get("height", 0)
                    print(f"      [{i}] ğŸ“… {date_str} | ğŸ“ {height:.2f}m")

                elif tbl_name in [
                    "total_calories_burned_record_table",
                    "active_calories_burned_record_table",
                ]:
                    energy = row.get("energy", 0) / 1000  # millikcal â†’ kcal
                    print(f"      [{i}] ğŸ“… {date_str} | ğŸ”¥ {energy:.0f}kcal")

                elif tbl_name == "oxygen_saturation_record_table":
                    percentage = row.get("percentage", 0)
                    print(f"      [{i}] ğŸ“… {date_str} | ğŸ« {percentage}%")

                else:
                    # ê¸°íƒ€ í…Œì´ë¸”
                    print(f"      [{i}] ğŸ“… {date_str}")

        else:
            # ì „ì²´ í•„ë“œ ëª¨ë“œ
            print(f"\n   ğŸ“Š ìƒ˜í”Œ ë°ì´í„° (ìµœì‹  {limit}ê°œ):")
            for i, row in enumerate(sorted_rows[:limit], 1):
                print(f"\n      [{i}]")
                for col, val in row.items():
                    val_str = str(val)
                    if len(val_str) > 50:
                        val_str = val_str[:50] + "..."
                    print(f"         {col}: {val_str}")

    print("\n" + "=" * 100)


def inspect_zip_parsed(zip_path: str):
    """
    ZIP â†’ ë‚ ì§œë³„ ì •ì œëœ ë°ì´í„°ë¡œ ë³€í™˜ í›„ í™•ì¸
    db_parser.pyê°€ ì‹¤ì œë¡œ ì¶”ì¶œí•˜ëŠ” ë°ì´í„° í™•ì¸

    Args:
        zip_path: ZIP íŒŒì¼ ê²½ë¡œ

    Returns:
        dict: ë‚ ì§œë³„ raw ë°ì´í„°
    """
    from app.core.unzipper import extract_zip_to_temp
    from app.core.db_to_json import db_to_json
    from app.core.db_parser import parse_db_json_to_raw_data_by_day
    from app.utils.preprocess import epoch_day_to_date_string

    print_header(f"ğŸ“¦ ZIP â†’ ì •ì œ ë°ì´í„° ë³€í™˜: {os.path.basename(zip_path)}")

    db_path = extract_zip_to_temp(zip_path)
    db_json = db_to_json(db_path)
    raw_by_day = parse_db_json_to_raw_data_by_day(db_json)

    if not raw_by_day:
        print("\nâŒ íŒŒì‹±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        print("   â†’ db_parser.pyê°€ ì¸ì‹í•˜ëŠ” í…Œì´ë¸”/ì»¬ëŸ¼ì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return None

    print(f"\nğŸ“… ì´ {len(raw_by_day)}ì¼ì¹˜ ë°ì´í„° ì¶”ì¶œë¨\n")

    # ë‚ ì§œ ì •ë ¬ (ìµœì‹ ìˆœ)
    sorted_dates = sorted(raw_by_day.keys(), reverse=True)

    # ì–´ë–¤ í•„ë“œì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ ì§‘ê³„
    field_stats = {}
    for date_int, raw in raw_by_day.items():
        for k, v in raw.items():
            if v and v != 0:
                field_stats[k] = field_stats.get(k, 0) + 1

    # ë°ì´í„° ìˆëŠ” í•„ë“œ ì¶œë ¥
    print(f"ğŸ“Š ë°ì´í„°ê°€ ìˆëŠ” í•„ë“œ (ì´ {len(field_stats)}ê°œ):")
    for field, count in sorted(field_stats.items(), key=lambda x: -x[1]):
        print(f"   {field:<25}: {count}ì¼")

    # ë‚ ì§œë³„ ìƒì„¸
    print(f"\nğŸ“… ë‚ ì§œë³„ ë°ì´í„° (ìµœê·¼ 10ì¼):")
    for date_int in sorted_dates[:10]:
        raw = raw_by_day[date_int]
        non_zero = {k: v for k, v in raw.items() if v and v != 0}

        # Epoch Day â†’ YYYY-MM-DD ë³€í™˜
        try:
            date_str = epoch_day_to_date_string(date_int)
        except:
            date_str = str(date_int)

        print(f"\n   {'â”€'*50}")
        print(f"   ğŸ“… {date_str}")
        if non_zero:
            for k, v in non_zero.items():
                print(f"      {k}: {format_data_value(k, v)}")
        else:
            print(f"      (ëª¨ë“  ê°’ì´ 0)")

    print("\n" + "=" * 100)
    return raw_by_day


def list_zip_files(directory: str = "./zip_data/uploads"):
    """
    ZIP íŒŒì¼ ëª©ë¡ ì¡°íšŒ

    Args:
        directory: ZIP íŒŒì¼ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬

    Returns:
        list: ZIP íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ìµœì‹ ìˆœ ì •ë ¬)
    """
    print_header(f"ğŸ“ ZIP íŒŒì¼ ëª©ë¡: {directory}")

    if not os.path.exists(directory):
        print(f"\nâŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}")
        return []

    zip_files = []
    for f in os.listdir(directory):
        if f.endswith(".zip"):
            path = os.path.join(directory, f)
            size = os.path.getsize(path)
            mtime = datetime.fromtimestamp(os.path.getmtime(path))
            zip_files.append({"name": f, "path": path, "size": size, "modified": mtime})

    if not zip_files:
        print("\nâš ï¸ ZIP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # ìˆ˜ì •ì¼ ê¸°ì¤€ ì •ë ¬
    zip_files.sort(key=lambda x: x["modified"], reverse=True)

    print(f"\nğŸ“Š ì´ {len(zip_files)}ê°œ íŒŒì¼\n")

    for i, zf in enumerate(zip_files, 1):
        size_kb = zf["size"] / 1024
        print(f"   [{i}] {zf['name']}")
        print(
            f"       í¬ê¸°: {size_kb:.1f} KB | ìˆ˜ì •: {zf['modified'].strftime('%Y-%m-%d %H:%M')}"
        )
        print(f"       ê²½ë¡œ: {zf['path']}")
        print()

    return zip_files


def get_latest_zip(directory: str = "./zip_data/uploads", user_id: str = None):
    """
    ê°€ì¥ ìµœê·¼ ZIP íŒŒì¼ ê²½ë¡œ ë°˜í™˜

    Args:
        directory: ZIP íŒŒì¼ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬
        user_id: íŠ¹ì • ì‚¬ìš©ì í•„í„° (ì˜ˆ: "11@aa.com" â†’ "11_aa_com" íŒ¨í„´ ë§¤ì¹­)

    Returns:
        str: ìµœì‹  ZIP íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
    """
    if not os.path.exists(directory):
        return None

    # user_idë¥¼ íŒŒì¼ëª… íŒ¨í„´ìœ¼ë¡œ ë³€í™˜
    user_pattern = None
    if user_id:
        user_pattern = user_id.replace("@", "_").replace(".", "_")

    zip_files = []
    for f in os.listdir(directory):
        if f.endswith(".zip"):
            # ì‚¬ìš©ì í•„í„°ë§
            if user_pattern and not f.startswith(user_pattern):
                continue

            path = os.path.join(directory, f)
            mtime = os.path.getmtime(path)
            zip_files.append({"path": path, "mtime": mtime, "name": f})

    if not zip_files:
        return None

    # ìµœì‹  íŒŒì¼ ë°˜í™˜
    zip_files.sort(key=lambda x: x["mtime"], reverse=True)
    return zip_files[0]["path"]


# ============================================================
# 2. VectorDB ì¡°íšŒ ê¸°ëŠ¥ (ê¸°ì¡´ ê¸°ëŠ¥)
# ============================================================


def get_date(user_id: str, date: str):
    """
    íŠ¹ì • ë‚ ì§œ ë°ì´í„° ì¡°íšŒ (í•¨ìˆ˜ë¡œ ì œê³µ)

    Args:
        user_id: ì‚¬ìš©ì ID
        date: ë‚ ì§œ (YYYY-MM-DD)

    Returns:
        dict: ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None

    Usage:
        from inspect_data import get_date
        data = get_date("user@example.com", "2025-12-16")
        print(data['summary_text'])
    """
    try:
        result = collection.get(where={"$and": [{"user_id": user_id}, {"date": date}]})

        if not result or not result["metadatas"]:
            return None

        metadata = result["metadatas"][0]
        summary_json = metadata.get("summary_json", "{}")

        try:
            summary_dict = json.loads(summary_json)
        except:
            summary_dict = {}

        return {
            "date": metadata.get("date"),
            "source": metadata.get("source"),
            "platform": metadata.get("platform"),
            "health_score": metadata.get("health_score"),
            "recommended_intensity": metadata.get("recommended_intensity"),
            "updated_at": metadata.get("updated_at"),
            "summary_text": summary_dict.get("summary_text", ""),
            "raw": summary_dict.get("raw", {}),
        }
    except Exception as e:
        print(f"[ERROR] {e}")
        return None


def view_specific_date(user_id: str, target_date: str):
    """íŠ¹ì • ë‚ ì§œ ë°ì´í„° ìƒì„¸ ì¶œë ¥"""
    print_header(f"ğŸ” íŠ¹ì • ë‚ ì§œ ì¡°íšŒ: {user_id} | {target_date}")

    data = get_date(user_id, target_date)

    if not data:
        print(f"\nâš ï¸ {target_date} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return

    print(f"\nğŸ“… ë‚ ì§œ: {data['date']}\n")

    # ë©”íƒ€ ì •ë³´
    print(f"ğŸ“Œ ë©”íƒ€ ì •ë³´:")
    print(f"   ì¶œì²˜(Source):     {data['source']}")
    print(f"   í”Œë«í¼(Platform): {data['platform']}")
    print(f"   ì—…ë°ì´íŠ¸:         {data['updated_at']}")
    print(f"   ê±´ê°• ì ìˆ˜:        {data['health_score']}ì ")
    print(f"   ê¶Œì¥ ê°•ë„:        {data['recommended_intensity']}\n")

    # ìš”ì•½
    if data["summary_text"]:
        print(f"ğŸ“ ìš”ì•½:")
        print(f"   {data['summary_text']}\n")

    # ìƒì„¸ ë°ì´í„° (0ì´ ì•„ë‹Œ ê°’ë§Œ)
    raw = data["raw"]
    if raw:
        print(f"ğŸ“Š ìƒì„¸ ë°ì´í„° (0ì´ ì•„ë‹Œ ê°’ë§Œ):\n")

        has_data = False
        for key, value in sorted(raw.items()):
            if value and value != 0:
                has_data = True
                formatted_value = format_data_value(key, value)
                print(f"   {key:25s}: {formatted_value}")

        if not has_data:
            print("   (ëª¨ë“  ê°’ì´ 0ì…ë‹ˆë‹¤)")

    print(f"\n{'='*100}\n")


def view_all_data(show_summary=False):
    """VectorDBì˜ ëª¨ë“  ë°ì´í„° í™•ì¸ (ì¶œì²˜ ë° í”Œë«í¼ í¬í•¨)"""
    print_header("ğŸ” VectorDB ì „ì²´ ë°ì´í„° ì¡°íšŒ")

    try:
        count = collection.count()
        print(f"\nğŸ“Š ì´ ì €ì¥ëœ ë°ì´í„°: {count}ê°œ\n")

        if count == 0:
            print("âš ï¸ VectorDBì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return

        # ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_data = collection.get()

        # ì‚¬ìš©ìë³„ ê·¸ë£¹í™”
        user_data = {}
        for metadata in all_data["metadatas"]:
            user_id = metadata.get("user_id", "unknown")
            if user_id not in user_data:
                user_data[user_id] = []
            user_data[user_id].append(metadata)

        # ì‚¬ìš©ìë³„ ì¶œë ¥
        for user_id, data_list in user_data.items():
            print(f"\nğŸ‘¤ User: {user_id}")
            print(f"   ë°ì´í„° ê°œìˆ˜: {len(data_list)}ê°œ\n")

            # ë‚ ì§œë³„ ì •ë ¬
            sorted_data = sorted(
                data_list, key=lambda x: x.get("date", ""), reverse=True
            )

            # ë‚ ì§œë³„ ê·¸ë£¹í™” (ê°™ì€ ë‚ ì§œ ì—¬ëŸ¬ ê±´ í™•ì¸)
            date_groups = {}
            for item in sorted_data:
                date = item.get("date", "unknown")
                if date not in date_groups:
                    date_groups[date] = []
                date_groups[date].append(item)

            # ë‚ ì§œë³„ ì¶œë ¥ (ìµœëŒ€ 20ê°œë§Œ)
            displayed = 0
            for date, items in list(date_groups.items())[:20]:
                if len(items) == 1:
                    item = items[0]
                    source = item.get("source", "unknown")
                    platform = item.get("platform", "unknown")
                    score = item.get("health_score", 0)
                    updated = item.get("updated_at", "")

                    print(f"   ğŸ“… {date}")
                    print(
                        f"      ì¶œì²˜: {source:15s} | í”Œë«í¼: {platform:8s} | ê±´ê°•ì ìˆ˜: {score}ì  | ì—…ë°ì´íŠ¸: {updated}"
                    )
                else:
                    # ê°™ì€ ë‚ ì§œì— ì—¬ëŸ¬ ê±´
                    print(f"   ğŸ“… {date} âš ï¸ ì¤‘ë³µ {len(items)}ê±´:")
                    for idx, item in enumerate(items, 1):
                        source = item.get("source", "unknown")
                        platform = item.get("platform", "unknown")
                        score = item.get("health_score", 0)
                        updated = item.get("updated_at", "")
                        print(
                            f"      [{idx}] ì¶œì²˜: {source:15s} | í”Œë«í¼: {platform:8s} | ê±´ê°•ì ìˆ˜: {score}ì  | ì—…ë°ì´íŠ¸: {updated}"
                        )

                displayed += 1

            if len(date_groups) > 20:
                print(f"\n   ... ì™¸ {len(date_groups) - 20}ê°œ ë‚ ì§œ")

            if show_summary:
                print(
                    f"\n   ğŸ’¡ ìƒì„¸ ë³´ê¸°: python inspect_data.py --user {user_id} --detail"
                )

        print("\n" + "=" * 100)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


def view_user_data(user_id: str, detailed=False, show_all_fields=False):
    """íŠ¹ì • ì‚¬ìš©ìì˜ ë°ì´í„° ìƒì„¸ ì¡°íšŒ (ì¶œì²˜ ë° í”Œë«í¼ í¬í•¨)"""
    print_header(f"ğŸ” User '{user_id}' ë°ì´í„° ìƒì„¸ ì¡°íšŒ")

    try:
        all_data = collection.get(where={"user_id": user_id})

        if not all_data or not all_data["metadatas"]:
            print("\nâš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return

        metadatas = all_data["metadatas"]
        print(f"\nğŸ“Š ì´ {len(metadatas)}ê°œ ë°ì´í„°\n")

        # ë‚ ì§œë³„ ì •ë ¬
        sorted_data = sorted(metadatas, key=lambda x: x.get("date", ""), reverse=True)

        for i, metadata in enumerate(sorted_data, 1):
            date = metadata.get("date", "unknown")
            source = metadata.get("source", "unknown")
            platform = metadata.get("platform", "unknown")
            updated = metadata.get("updated_at", "")
            health_score = metadata.get("health_score", 0)
            intensity = metadata.get("recommended_intensity", "ì¤‘")

            # summary_json íŒŒì‹±
            summary_json = metadata.get("summary_json", "{}")
            try:
                summary_dict = json.loads(summary_json)
            except:
                summary_dict = {}

            raw = summary_dict.get("raw", {})
            summary_text = summary_dict.get("summary_text", "")

            print(f"\n{'='*100}")
            print(f"ğŸ“… [{i}] ë‚ ì§œ: {date}")
            print(f"{'='*100}")

            # ë©”íƒ€ ì •ë³´
            print(f"\nğŸ“Œ ë©”íƒ€ ì •ë³´:")
            print(f"   ì¶œì²˜(Source):     {source}")
            print(f"   í”Œë«í¼(Platform): {platform}")
            print(f"   ì—…ë°ì´íŠ¸:         {updated}")
            print(f"   ê±´ê°• ì ìˆ˜:        {health_score}ì ")
            print(f"   ê¶Œì¥ ê°•ë„:        {intensity}")

            # ìš”ì•½ í…ìŠ¤íŠ¸
            if summary_text:
                print(f"\nğŸ“ ìš”ì•½:")
                if len(summary_text) > 200:
                    print(f"   {summary_text[:200]}...")
                else:
                    print(f"   {summary_text}")

            if detailed and raw:
                # ìƒì„¸ ë°ì´í„° ì¶œë ¥
                print(f"\nğŸ“Š ìƒì„¸ ë°ì´í„° ({len(raw)}ê°œ í•­ëª©):")

                if show_all_fields:
                    # ëª¨ë“  í•„ë“œ ì¶œë ¥
                    for key, value in sorted(raw.items()):
                        formatted_value = format_data_value(key, value)
                        print(f"   {key:25s}: {formatted_value}")
                else:
                    # ì£¼ìš” í•„ë“œë§Œ ì¶œë ¥
                    key_fields = [
                        "sleep_hr",
                        "sleep_min",
                        "steps",
                        "distance_km",
                        "active_calories",
                        "heart_rate",
                        "resting_heart_rate",
                        "weight",
                        "bmi",
                        "exercise_min",
                    ]

                    available_fields = [k for k in key_fields if k in raw]
                    other_fields = [k for k in raw.keys() if k not in key_fields]

                    if available_fields:
                        print("\n   ğŸ“Œ ì£¼ìš” ì§€í‘œ:")
                        for key in available_fields:
                            value = raw[key]
                            if value and value != 0:
                                formatted_value = format_data_value(key, value)
                                print(f"      {key:25s}: {formatted_value}")

                    if other_fields:
                        non_zero_others = [
                            k for k in other_fields if raw.get(k) and raw.get(k) != 0
                        ]
                        if non_zero_others:
                            print(
                                f"\n   ğŸ’¡ ê¸°íƒ€ {len(non_zero_others)}ê°œ í•­ëª©: {', '.join(non_zero_others[:5])}..."
                            )
                            print(f"      (ì „ì²´ ë³´ê¸°: --all-fields ì˜µì…˜ ì‚¬ìš©)")

        print("\n" + "=" * 100)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


def check_duplicates(user_id: str = None):
    """ê°™ì€ ë‚ ì§œ ì¤‘ë³µ ë°ì´í„° í™•ì¸"""
    print_header("ğŸ” ì¤‘ë³µ ë°ì´í„° í™•ì¸")

    try:
        all_data = collection.get()

        if user_id:
            filtered_metadatas = [
                m for m in all_data["metadatas"] if m.get("user_id") == user_id
            ]
        else:
            filtered_metadatas = all_data["metadatas"]

        # ë‚ ì§œë³„ ê·¸ë£¹í™”
        date_groups = {}
        for metadata in filtered_metadatas:
            uid = metadata.get("user_id", "unknown")
            date = metadata.get("date", "unknown")
            key = f"{uid}_{date}"

            if key not in date_groups:
                date_groups[key] = []
            date_groups[key].append(metadata)

        # ì¤‘ë³µ ì°¾ê¸°
        duplicates = {k: v for k, v in date_groups.items() if len(v) > 1}

        if not duplicates:
            print("\nâœ… ì¤‘ë³µ ë°ì´í„° ì—†ìŒ!")
            return

        print(f"\nâš ï¸ ì´ {len(duplicates)}ê°œ ë‚ ì§œì— ì¤‘ë³µ ë°ì´í„° ë°œê²¬:\n")

        for key, items in duplicates.items():
            user_id, date = key.split("_", 1)
            print(f"\nğŸ‘¤ User: {user_id} | ğŸ“… ë‚ ì§œ: {date} | ì¤‘ë³µ: {len(items)}ê±´")

            for idx, item in enumerate(items, 1):
                source = item.get("source", "unknown")
                platform = item.get("platform", "unknown")
                updated = item.get("updated_at", "")
                score = item.get("health_score", 0)
                print(
                    f"   [{idx}] ì¶œì²˜: {source:15s} | í”Œë«í¼: {platform:8s} | ê±´ê°•ì ìˆ˜: {score}ì  | ì—…ë°ì´íŠ¸: {updated}"
                )

        print("\n" + "=" * 100)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


# ============================================================
# 3. ë°ì´í„° ì‚­ì œ ê¸°ëŠ¥
# ============================================================


def delete_user_data(user_id: str, confirm: bool = False):
    """íŠ¹ì • ì‚¬ìš©ìì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
    print_header(f"ğŸ—‘ï¸ ì‚¬ìš©ì ë°ì´í„° ì‚­ì œ: {user_id}")

    try:
        result = collection.get(where={"user_id": user_id})
        ids = result.get("ids", [])

        if not ids:
            print(f"\nâš ï¸ '{user_id}' ì‚¬ìš©ìì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"\nğŸ“Š ì‚­ì œ ëŒ€ìƒ: {len(ids)}ê°œ ë ˆì½”ë“œ")

        if not confirm:
            print("\nâš ï¸ ì‹¤ì œ ì‚­ì œí•˜ë ¤ë©´ --confirm ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.")
            print(f"   ì˜ˆ: python inspect_data.py --delete-user {user_id} --confirm")
            return

        collection.delete(ids=ids)
        print(f"\nâœ… {len(ids)}ê°œ ë ˆì½”ë“œ ì‚­ì œ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


def delete_old_format_data(user_id: str = None, confirm: bool = False):
    """
    ì˜ˆì „ í˜•ì‹ ë°ì´í„° ì‚­ì œ (sourceê°€ 'api', 'zip' ë“± í”Œë«í¼ ì—†ëŠ” ê²ƒ)
    ìƒˆ í˜•ì‹: 'api_samsung', 'api_apple', 'zip_samsung', 'zip_apple'
    """
    print_header("ğŸ—‘ï¸ ì˜ˆì „ í˜•ì‹ ë°ì´í„° ì‚­ì œ")

    try:
        if user_id:
            result = collection.get(where={"user_id": user_id})
        else:
            result = collection.get()

        ids = result.get("ids", [])
        metadatas = result.get("metadatas", [])

        # ì˜ˆì „ í˜•ì‹ ì°¾ê¸° (sourceê°€ í”Œë«í¼ ì •ë³´ ì—†ëŠ” ê²ƒ)
        old_format_sources = ["api", "zip", "unknown", None, ""]

        to_delete = []
        for doc_id, meta in zip(ids, metadatas):
            source = meta.get("source", "")
            platform = meta.get("platform", "")

            # ì˜ˆì „ í˜•ì‹ ì¡°ê±´
            is_old = (
                source in old_format_sources
                or platform in ["unknown", None, ""]
                or (source and "_" not in source)  # api_samsung í˜•ì‹ì´ ì•„ë‹Œ ê²ƒ
            )

            if is_old:
                to_delete.append(
                    {
                        "id": doc_id,
                        "user_id": meta.get("user_id"),
                        "date": meta.get("date"),
                        "source": source,
                        "platform": platform,
                    }
                )

        if not to_delete:
            print("\nâœ… ì˜ˆì „ í˜•ì‹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return

        print(f"\nğŸ“Š ì‚­ì œ ëŒ€ìƒ: {len(to_delete)}ê°œ ë ˆì½”ë“œ\n")

        # ì‚¬ìš©ìë³„ ê·¸ë£¹í™”í•´ì„œ ì¶œë ¥
        by_user = {}
        for item in to_delete:
            uid = item["user_id"]
            if uid not in by_user:
                by_user[uid] = []
            by_user[uid].append(item)

        for uid, items in by_user.items():
            print(f"ğŸ‘¤ {uid}: {len(items)}ê°œ")
            for item in items[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                print(
                    f"   ğŸ“… {item['date']} | ì¶œì²˜: {item['source']} | í”Œë«í¼: {item['platform']}"
                )
            if len(items) > 5:
                print(f"   ... ì™¸ {len(items) - 5}ê°œ")

        if not confirm:
            print("\nâš ï¸ ì‹¤ì œ ì‚­ì œí•˜ë ¤ë©´ --confirm ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.")
            print(f"   ì˜ˆ: python inspect_data.py --delete-old --confirm")
            return

        # ì‚­ì œ ì‹¤í–‰
        delete_ids = [item["id"] for item in to_delete]
        collection.delete(ids=delete_ids)
        print(f"\nâœ… {len(delete_ids)}ê°œ ë ˆì½”ë“œ ì‚­ì œ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


def delete_by_source(source: str, user_id: str = None, confirm: bool = False):
    """íŠ¹ì • ì¶œì²˜(source)ì˜ ë°ì´í„° ì‚­ì œ"""
    print_header(f"ğŸ—‘ï¸ ì¶œì²˜ë³„ ë°ì´í„° ì‚­ì œ: {source}")

    try:
        if user_id:
            result = collection.get(
                where={"$and": [{"user_id": user_id}, {"source": source}]}
            )
        else:
            result = collection.get(where={"source": source})

        ids = result.get("ids", [])
        metadatas = result.get("metadatas", [])

        if not ids:
            print(f"\nâš ï¸ ì¶œì²˜ '{source}'ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"\nğŸ“Š ì‚­ì œ ëŒ€ìƒ: {len(ids)}ê°œ ë ˆì½”ë“œ\n")

        for meta in metadatas[:10]:
            print(
                f"   ğŸ“… {meta.get('date')} | ì‚¬ìš©ì: {meta.get('user_id')} | í”Œë«í¼: {meta.get('platform')}"
            )
        if len(metadatas) > 10:
            print(f"   ... ì™¸ {len(metadatas) - 10}ê°œ")

        if not confirm:
            print("\nâš ï¸ ì‹¤ì œ ì‚­ì œí•˜ë ¤ë©´ --confirm ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.")
            return

        collection.delete(ids=ids)
        print(f"\nâœ… {len(ids)}ê°œ ë ˆì½”ë“œ ì‚­ì œ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


def check_chroma_location():
    """ChromaDB ì €ì¥ ìœ„ì¹˜ í™•ì¸"""
    print_header("ğŸ“ ChromaDB ì €ì¥ ìœ„ì¹˜ í™•ì¸")

    try:
        chroma_dir = "./chroma_data"
        abs_path = os.path.abspath(chroma_dir)

        print(f"\nê²½ë¡œ: {abs_path}")

        if os.path.exists(abs_path):
            print(f"ìƒíƒœ: âœ… ì¡´ì¬í•¨")

            # ë””ë ‰í† ë¦¬ í¬ê¸°
            total_size = 0
            file_count = 0
            for root, dirs, files in os.walk(abs_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    total_size += os.path.getsize(file_path)
                    file_count += 1

            print(f"íŒŒì¼: {file_count}ê°œ")
            print(
                f"í¬ê¸°: {total_size / 1024:.2f} KB ({total_size / (1024*1024):.2f} MB)"
            )
        else:
            print(f"ìƒíƒœ: âŒ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")

        print("\n" + "=" * 100)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def list_users():
    """VectorDBì— ì €ì¥ëœ ì‚¬ìš©ì(ì´ë©”ì¼) ëª©ë¡ ì¡°íšŒ"""
    print_header("ğŸ‘¥ ì‚¬ìš©ì ëª©ë¡")

    try:
        all_data = collection.get()
        metadatas = all_data.get("metadatas", [])

        if not metadatas:
            print("\nâš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return

        # ì‚¬ìš©ìë³„ ì§‘ê³„
        user_stats = {}
        for meta in metadatas:
            user_id = meta.get("user_id", "unknown")
            source = meta.get("source", "unknown")
            platform = meta.get("platform", "unknown")

            if user_id not in user_stats:
                user_stats[user_id] = {
                    "count": 0,
                    "sources": set(),
                    "platforms": set(),
                }

            user_stats[user_id]["count"] += 1
            user_stats[user_id]["sources"].add(source)
            user_stats[user_id]["platforms"].add(platform)

        print(f"\nğŸ“Š ì´ {len(user_stats)}ëª…ì˜ ì‚¬ìš©ì\n")

        # ë°ì´í„° ê°œìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_users = sorted(user_stats.items(), key=lambda x: -x[1]["count"])

        for i, (user_id, stats) in enumerate(sorted_users, 1):
            sources = ", ".join(stats["sources"])
            platforms = ", ".join(stats["platforms"])
            print(f"   [{i}] {user_id}")
            print(
                f"       ë°ì´í„°: {stats['count']}ê°œ | ì¶œì²˜: {sources} | í”Œë«í¼: {platforms}"
            )
            print()

        print("=" * 100)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


def show_date_range(user_id: str = None):
    """ë‚ ì§œ ë²”ìœ„ í™•ì¸"""
    print_header("ğŸ“… ë‚ ì§œ ë²”ìœ„ í™•ì¸")

    try:
        if user_id:
            all_data = collection.get(where={"user_id": user_id})
        else:
            all_data = collection.get()

        metadatas = all_data.get("metadatas", [])

        if not metadatas:
            print("\nâš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return

        dates = [m.get("date") for m in metadatas if m.get("date")]
        dates = sorted(set(dates))

        if dates:
            print(f"\nğŸ“Š ì´ {len(dates)}ê°œ ë‚ ì§œ")
            print(f"ğŸ“… ë²”ìœ„: {dates[0]} ~ {dates[-1]}")
            print(f"\nìµœê·¼ 10ê°œ ë‚ ì§œ:")
            for date in dates[-10:]:
                date_items = [m for m in metadatas if m.get("date") == date]
                sources = set([m.get("source", "unknown") for m in date_items])
                platforms = set([m.get("platform", "unknown") for m in date_items])
                print(
                    f"   {date} | ê±´ìˆ˜: {len(date_items)} | ì¶œì²˜: {', '.join(sources)} | í”Œë«í¼: {', '.join(platforms)}"
                )

        print("\n" + "=" * 100)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


# ============================================================
# ë©”ì¸ (CLI)
# ============================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="ë°ì´í„° ê²€ì‚¬ í†µí•© ìŠ¤í¬ë¦½íŠ¸ (VectorDB + ZIP ë¶„ì„)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“¦ ZIP íŒŒì¼ ë¶„ì„
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  python inspect_data.py --zip-list                     # ZIP íŒŒì¼ ëª©ë¡
  python inspect_data.py --zip-latest                   # ìµœì‹  ZIP í…Œì´ë¸” ìš”ì•½
  python inspect_data.py --zip-latest --parsed          # ìµœì‹  ZIP ì •ì œ ë°ì´í„°
  python inspect_data.py --zip-latest --table <í…Œì´ë¸”>  # ìµœì‹  ZIP íŠ¹ì • í…Œì´ë¸”
  python inspect_data.py --zip <ê²½ë¡œ>                   # íŠ¹ì • ZIP í…Œì´ë¸” ìš”ì•½
  python inspect_data.py --zip <ê²½ë¡œ> --table <í…Œì´ë¸”>  # íŠ¹ì • ZIP íŠ¹ì • í…Œì´ë¸”
  python inspect_data.py --zip <ê²½ë¡œ> --parsed          # íŠ¹ì • ZIP ì •ì œ ë°ì´í„°

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ—„ï¸ VectorDB ì¡°íšŒ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  python inspect_data.py --all                          # ì „ì²´ ë°ì´í„° ìš”ì•½
  python inspect_data.py --user <ì´ë©”ì¼>                # íŠ¹ì • ì‚¬ìš©ì ë°ì´í„°
  python inspect_data.py --user <ì´ë©”ì¼> --detail       # ìƒì„¸ ì •ë³´
  python inspect_data.py --user <ì´ë©”ì¼> --detail --all-fields  # ëª¨ë“  í•„ë“œ
  python inspect_data.py --date <YYYY-MM-DD> --user <ì´ë©”ì¼>    # íŠ¹ì • ë‚ ì§œ
  python inspect_data.py --duplicates                   # ì¤‘ë³µ ë°ì´í„° í™•ì¸
  python inspect_data.py --dates                        # ë‚ ì§œ ë²”ìœ„ í™•ì¸
  python inspect_data.py --location                     # ChromaDB ìœ„ì¹˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ Pythonì—ì„œ importí•´ì„œ ì‚¬ìš©
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  from inspect_data import get_date, inspect_zip, get_latest_zip
  
  # VectorDB ì¡°íšŒ
  data = get_date("user@example.com", "2025-12-16")
  print(data['raw'])
  
  # ZIP ë¶„ì„
  latest = get_latest_zip()  # ìµœì‹  ZIP ê²½ë¡œ
  db_json = inspect_zip(latest)
        """,
    )

    # ZIP ê´€ë ¨ ì¸ì
    parser.add_argument("--zip", type=str, help="ZIP íŒŒì¼ ê²½ë¡œ (í…Œì´ë¸” ìš”ì•½)")
    parser.add_argument("--zip-list", action="store_true", help="ZIP íŒŒì¼ ëª©ë¡ ì¡°íšŒ")
    parser.add_argument(
        "--zip-latest", action="store_true", help="ê°€ì¥ ìµœê·¼ ZIP íŒŒì¼ ë¶„ì„"
    )
    parser.add_argument(
        "--table", type=str, help="íŠ¹ì • í…Œì´ë¸” ìƒì„¸ (--zipê³¼ í•¨ê»˜ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--summary", action="store_true", help="í•µì‹¬ ë°ì´í„°ë§Œ í‘œì‹œ (ë‚ ì§œ, ê°’)"
    )
    parser.add_argument(
        "--parsed", action="store_true", help="ì •ì œëœ ë°ì´í„° í™•ì¸ (--zipê³¼ í•¨ê»˜ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--limit", type=int, default=5, help="ìƒ˜í”Œ ë°ì´í„° ê°œìˆ˜ (ê¸°ë³¸: 5)"
    )

    # VectorDB ê´€ë ¨ ì¸ì
    parser.add_argument("--all", action="store_true", help="ì „ì²´ ë°ì´í„° ìš”ì•½ ë³´ê¸°")
    parser.add_argument("--users", action="store_true", help="ì‚¬ìš©ì(ì´ë©”ì¼) ëª©ë¡ ë³´ê¸°")
    parser.add_argument("--user", type=str, help="íŠ¹ì • ì‚¬ìš©ì ë°ì´í„° í™•ì¸")
    parser.add_argument("--date", type=str, help="íŠ¹ì • ë‚ ì§œ ì¡°íšŒ (YYYY-MM-DD)")
    parser.add_argument("--detail", action="store_true", help="ìƒì„¸ ì •ë³´ ë³´ê¸°")
    parser.add_argument(
        "--all-fields", action="store_true", help="ëª¨ë“  ë°ì´í„° í•„ë“œ ë³´ê¸°"
    )
    parser.add_argument("--duplicates", action="store_true", help="ì¤‘ë³µ ë°ì´í„° í™•ì¸")
    parser.add_argument("--dates", action="store_true", help="ë‚ ì§œ ë²”ìœ„ í™•ì¸")
    parser.add_argument("--location", action="store_true", help="ChromaDB ìœ„ì¹˜ í™•ì¸")

    # ì‚­ì œ ê´€ë ¨ ì¸ì
    parser.add_argument("--delete-user", type=str, help="íŠ¹ì • ì‚¬ìš©ì ë°ì´í„° ì‚­ì œ")
    parser.add_argument(
        "--delete-old",
        action="store_true",
        help="ì˜ˆì „ í˜•ì‹ ë°ì´í„° ì‚­ì œ (sourceì— í”Œë«í¼ ì—†ëŠ” ê²ƒ)",
    )
    parser.add_argument(
        "--delete-source", type=str, help="íŠ¹ì • ì¶œì²˜ ë°ì´í„° ì‚­ì œ (ì˜ˆ: api, zip)"
    )
    parser.add_argument("--confirm", action="store_true", help="ì‚­ì œ ì‹¤í–‰ í™•ì¸")

    args = parser.parse_args()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ZIP ë¶„ì„
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.zip_list:
        list_zip_files()
    elif args.zip_latest:
        # ê°€ì¥ ìµœê·¼ ZIP íŒŒì¼ ìë™ ì„ íƒ
        latest_path = get_latest_zip()
        if not latest_path:
            print("âŒ ZIP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"ğŸ“¦ ìµœì‹  ZIP: {os.path.basename(latest_path)}\n")
            if args.parsed:
                inspect_zip_parsed(latest_path)
            elif args.table:
                inspect_zip_table(latest_path, args.table, args.limit, args.summary)
            else:
                inspect_zip(latest_path)
    elif args.zip:
        if args.parsed:
            inspect_zip_parsed(args.zip)
        elif args.table:
            inspect_zip_table(args.zip, args.table, args.limit, args.summary)
        else:
            inspect_zip(args.zip)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # VectorDB ì¡°íšŒ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elif args.date and args.user:
        view_specific_date(args.user, args.date)
    elif args.all:
        view_all_data(show_summary=True)
    elif args.users:
        list_users()
    elif args.user:
        view_user_data(args.user, detailed=args.detail, show_all_fields=args.all_fields)
    elif args.duplicates:
        check_duplicates()
    elif args.dates:
        show_date_range()
    elif args.location:
        check_chroma_location()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì‚­ì œ ê¸°ëŠ¥
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elif args.delete_user:
        delete_user_data(args.delete_user, confirm=args.confirm)
    elif args.delete_old:
        delete_old_format_data(user_id=args.user, confirm=args.confirm)
    elif args.delete_source:
        delete_by_source(args.delete_source, user_id=args.user, confirm=args.confirm)

    else:
        # ê¸°ë³¸: ë„ì›€ë§ + ì „ì²´ ìš”ì•½
        check_chroma_location()
        print()
        view_all_data(show_summary=True)
